Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.532917     228 container_manager_linux.go:301] "Creating device plugin manager"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.532935     228 state_mem.go:36] "Initialized new in-memory state store"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.532979     228 kubelet.go:401] "Attempting to sync node with API server"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.532984     228 kubelet.go:302] "Adding static pod path" path="/etc/kubernetes/manifests"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.532996     228 kubelet.go:313] "Adding apiserver pod source"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.533004     228 apiserver.go:42] "Waiting for node sync before watching apiserver pods"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.533349     228 kuberuntime_manager.go:261] "Container runtime initialized" containerRuntime="containerd" version="v1.7.18" apiVersion="v1"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.533406     228 kubelet.go:816] "Not starting ClusterTrustBundle informer because we are in static kubelet mode"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.533980     228 server.go:1264] "Started kubelet"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.536499     228 server.go:163] "Starting to listen" address="0.0.0.0" port=10250
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.537213     228 server.go:455] "Adding debug handlers to kubelet server"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.537399     228 fs_resource_analyzer.go:67] "Starting FS ResourceAnalyzer"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.538071     228 volume_manager.go:291] "Starting Kubelet Volume Manager"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.538171     228 desired_state_of_world_populator.go:149] "Desired state populator starts to run"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.538228     228 reconciler.go:26] "Reconciler: start to sync state"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.538222     228 ratelimit.go:55] "Setting rate limiting for endpoint" service="podresources" qps=100 burstTokens=10
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.538335     228 server.go:227] "Starting to serve the podresources API" endpoint="unix:/var/lib/kubelet/pod-resources/kubelet.sock"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.539188     228 factory.go:219] Registration of the crio container factory failed: Get "http://%2Fvar%2Frun%2Fcrio%2Fcrio.sock/info": dial unix /var/run/crio/crio.sock: connect: no such file or directory
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.543223     228 factory.go:221] Registration of the containerd container factory successfully
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.543273     228 factory.go:221] Registration of the systemd container factory successfully
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.546618     228 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv4"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.547964     228 kubelet_network_linux.go:50] "Initialized iptables rules." protocol="IPv6"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.548032     228 status_manager.go:217] "Starting to sync pod status with apiserver"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.548043     228 kubelet.go:2356] "Starting kubelet main sync loop"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: E1228 16:05:02.548061     228 kubelet.go:2380] "Skipping pod synchronization" err="[container runtime status check may not have completed yet, PLEG is not healthy: pleg has yet to be successful]"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.548068     228 cpu_manager.go:214] "Starting CPU manager" policy="none"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.548073     228 cpu_manager.go:215] "Reconciling" reconcilePeriod="10s"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.548082     228 state_mem.go:36] "Initialized new in-memory state store"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.548149     228 state_mem.go:88] "Updated default CPUSet" cpuSet=""
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.548154     228 state_mem.go:96] "Updated CPUSet assignments" assignments={}
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.548164     228 policy_none.go:49] "None policy: Start"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.548479     228 memory_manager.go:170] "Starting memorymanager" policy="None"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.548493     228 state_mem.go:35] "Initializing new in-memory state store"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.548576     228 state_mem.go:75] "Updated machine memory state"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: E1228 16:05:02.554181     228 nodelease.go:49] "Failed to get node when trying to set owner ref to the node lease" err="nodes \"api-server-failure-worker2\" not found" node="api-server-failure-worker2"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.573428     228 manager.go:479] "Failed to read data from checkpoint" checkpoint="kubelet_internal_checkpoint" err="checkpoint is not found"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.573481     228 container_log_manager.go:186] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.573514     228 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: E1228 16:05:02.573970     228 eviction_manager.go:282] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"api-server-failure-worker2\" not found"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.640326     228 kubelet_node_status.go:73] "Attempting to register node" node="api-server-failure-worker2"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.643131     228 kubelet_node_status.go:76] "Successfully registered node" node="api-server-failure-worker2"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.648870     228 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.3.0/24"
Dec 28 16:05:02 api-server-failure-worker2 kubelet[228]: I1228 16:05:02.649745     228 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.3.0/24"
Dec 28 16:05:03 api-server-failure-worker2 kubelet[228]: I1228 16:05:03.534013     228 apiserver.go:52] "Watching apiserver"
Dec 28 16:05:03 api-server-failure-worker2 kubelet[228]: I1228 16:05:03.534952     228 topology_manager.go:215] "Topology Admit Handler" podUID="2ad1e530-be0a-4366-b74e-e53e3733498b" podNamespace="kube-system" podName="kindnet-4x5zz"
Dec 28 16:05:03 api-server-failure-worker2 kubelet[228]: I1228 16:05:03.535010     228 topology_manager.go:215] "Topology Admit Handler" podUID="13056f49-ee24-45c3-9865-5be865d69d09" podNamespace="kube-system" podName="kube-proxy-tgvdx"
Dec 28 16:05:03 api-server-failure-worker2 kubelet[228]: I1228 16:05:03.540099     228 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Dec 28 16:05:03 api-server-failure-worker2 kubelet[228]: I1228 16:05:03.548225     228 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/2ad1e530-be0a-4366-b74e-e53e3733498b-cni-cfg\") pod \"kindnet-4x5zz\" (UID: \"2ad1e530-be0a-4366-b74e-e53e3733498b\") " pod="kube-system/kindnet-4x5zz"
Dec 28 16:05:03 api-server-failure-worker2 kubelet[228]: I1228 16:05:03.548298     228 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/2ad1e530-be0a-4366-b74e-e53e3733498b-xtables-lock\") pod \"kindnet-4x5zz\" (UID: \"2ad1e530-be0a-4366-b74e-e53e3733498b\") " pod="kube-system/kindnet-4x5zz"
Dec 28 16:05:03 api-server-failure-worker2 kubelet[228]: I1228 16:05:03.548325     228 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/2ad1e530-be0a-4366-b74e-e53e3733498b-lib-modules\") pod \"kindnet-4x5zz\" (UID: \"2ad1e530-be0a-4366-b74e-e53e3733498b\") " pod="kube-system/kindnet-4x5zz"
Dec 28 16:05:03 api-server-failure-worker2 kubelet[228]: I1228 16:05:03.548346     228 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-pvgh5\" (UniqueName: \"kubernetes.io/projected/2ad1e530-be0a-4366-b74e-e53e3733498b-kube-api-access-pvgh5\") pod \"kindnet-4x5zz\" (UID: \"2ad1e530-be0a-4366-b74e-e53e3733498b\") " pod="kube-system/kindnet-4x5zz"
Dec 28 16:05:03 api-server-failure-worker2 kubelet[228]: I1228 16:05:03.548367     228 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/13056f49-ee24-45c3-9865-5be865d69d09-kube-proxy\") pod \"kube-proxy-tgvdx\" (UID: \"13056f49-ee24-45c3-9865-5be865d69d09\") " pod="kube-system/kube-proxy-tgvdx"
Dec 28 16:05:03 api-server-failure-worker2 kubelet[228]: I1228 16:05:03.548382     228 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/13056f49-ee24-45c3-9865-5be865d69d09-xtables-lock\") pod \"kube-proxy-tgvdx\" (UID: \"13056f49-ee24-45c3-9865-5be865d69d09\") " pod="kube-system/kube-proxy-tgvdx"
Dec 28 16:05:03 api-server-failure-worker2 kubelet[228]: I1228 16:05:03.548401     228 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/13056f49-ee24-45c3-9865-5be865d69d09-lib-modules\") pod \"kube-proxy-tgvdx\" (UID: \"13056f49-ee24-45c3-9865-5be865d69d09\") " pod="kube-system/kube-proxy-tgvdx"
Dec 28 16:05:03 api-server-failure-worker2 kubelet[228]: I1228 16:05:03.548424     228 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-f4khb\" (UniqueName: \"kubernetes.io/projected/13056f49-ee24-45c3-9865-5be865d69d09-kube-api-access-f4khb\") pod \"kube-proxy-tgvdx\" (UID: \"13056f49-ee24-45c3-9865-5be865d69d09\") " pod="kube-system/kube-proxy-tgvdx"
Dec 28 16:05:05 api-server-failure-worker2 kubelet[228]: I1228 16:05:05.574482     228 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kindnet-4x5zz" podStartSLOduration=3.574439632 podStartE2EDuration="3.574439632s" podCreationTimestamp="2024-12-28 16:05:02 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-12-28 16:05:05.574442132 +0000 UTC m=+3.080159544" watchObservedRunningTime="2024-12-28 16:05:05.574439632 +0000 UTC m=+3.080156961"
Dec 28 16:05:05 api-server-failure-worker2 kubelet[228]: I1228 16:05:05.574755     228 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-tgvdx" podStartSLOduration=3.574744882 podStartE2EDuration="3.574744882s" podCreationTimestamp="2024-12-28 16:05:02 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-12-28 16:05:04.555394173 +0000 UTC m=+2.061111544" watchObservedRunningTime="2024-12-28 16:05:05.574744882 +0000 UTC m=+3.080462253"
Dec 28 16:05:15 api-server-failure-worker2 kubelet[228]: I1228 16:05:15.002330     228 kubelet_node_status.go:497] "Fast updating node status as it just became ready"
Dec 28 16:08:56 api-server-failure-worker2 kubelet[228]: E1228 16:08:56.965332     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?resourceVersion=0&timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused"
Dec 28 16:08:56 api-server-failure-worker2 kubelet[228]: E1228 16:08:56.965996     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused"
Dec 28 16:08:56 api-server-failure-worker2 kubelet[228]: E1228 16:08:56.966497     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused"
Dec 28 16:08:56 api-server-failure-worker2 kubelet[228]: E1228 16:08:56.966937     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused"
Dec 28 16:08:56 api-server-failure-worker2 kubelet[228]: E1228 16:08:56.967346     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused"
Dec 28 16:08:56 api-server-failure-worker2 kubelet[228]: E1228 16:08:56.967361     228 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
Dec 28 16:08:57 api-server-failure-worker2 kubelet[228]: E1228 16:08:57.706615     228 controller.go:195] "Failed to update lease" err="Put \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused"
Dec 28 16:08:57 api-server-failure-worker2 kubelet[228]: E1228 16:08:57.707112     228 controller.go:195] "Failed to update lease" err="Put \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused"
Dec 28 16:08:57 api-server-failure-worker2 kubelet[228]: E1228 16:08:57.707586     228 controller.go:195] "Failed to update lease" err="Put \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused"
Dec 28 16:08:57 api-server-failure-worker2 kubelet[228]: E1228 16:08:57.708066     228 controller.go:195] "Failed to update lease" err="Put \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused"
Dec 28 16:08:57 api-server-failure-worker2 kubelet[228]: E1228 16:08:57.708563     228 controller.go:195] "Failed to update lease" err="Put \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused"
Dec 28 16:08:57 api-server-failure-worker2 kubelet[228]: I1228 16:08:57.708586     228 controller.go:115] "failed to update lease using latest lease, fallback to ensure lease" err="failed 5 attempts to update lease"
Dec 28 16:08:57 api-server-failure-worker2 kubelet[228]: E1228 16:08:57.709017     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused" interval="200ms"
Dec 28 16:08:57 api-server-failure-worker2 kubelet[228]: E1228 16:08:57.911370     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused" interval="400ms"
Dec 28 16:08:58 api-server-failure-worker2 kubelet[228]: E1228 16:08:58.314857     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused" interval="800ms"
Dec 28 16:08:59 api-server-failure-worker2 kubelet[228]: E1228 16:08:59.116926     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused" interval="1.6s"
Dec 28 16:09:00 api-server-failure-worker2 kubelet[228]: E1228 16:09:00.718836     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused" interval="3.2s"
Dec 28 16:09:03 api-server-failure-worker2 kubelet[228]: E1228 16:09:03.921609     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp 172.18.0.4:6443: connect: connection refused" interval="6.4s"
Dec 28 16:09:07 api-server-failure-worker2 kubelet[228]: E1228 16:09:07.309294     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: Get "https://api-server-failure-control-plane:6443/apis/node.k8s.io/v1/runtimeclasses?allowWatchBookmarks=true&resourceVersion=1006&timeout=5m10s&timeoutSeconds=310&watch=true": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:07 api-server-failure-worker2 kubelet[228]: E1228 16:09:07.309451     228 reflector.go:150] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=1004&timeout=9m3s&timeoutSeconds=543&watch=true": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:07 api-server-failure-worker2 kubelet[228]: E1228 16:09:07.356228     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?resourceVersion=0&timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:07 api-server-failure-worker2 kubelet[228]: E1228 16:09:07.356331     228 reflector.go:150] object-"kube-system"/"kube-proxy": Failed to watch *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=1004&timeout=6m28s&timeoutSeconds=388&watch=true": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:07 api-server-failure-worker2 kubelet[228]: E1228 16:09:07.363317     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:07 api-server-failure-worker2 kubelet[228]: E1228 16:09:07.369529     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:07 api-server-failure-worker2 kubelet[228]: E1228 16:09:07.376357     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: Get "https://api-server-failure-control-plane:6443/api/v1/services?allowWatchBookmarks=true&resourceVersion=1003&timeout=5m27s&timeoutSeconds=327&watch=true": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:07 api-server-failure-worker2 kubelet[228]: E1228 16:09:07.376377     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:07 api-server-failure-worker2 kubelet[228]: E1228 16:09:07.379659     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:07 api-server-failure-worker2 kubelet[228]: E1228 16:09:07.379685     228 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
Dec 28 16:09:07 api-server-failure-worker2 kubelet[228]: E1228 16:09:07.451597     228 reflector.go:150] pkg/kubelet/config/apiserver.go:66: Failed to watch *v1.Pod: Get "https://api-server-failure-control-plane:6443/api/v1/pods?allowWatchBookmarks=true&fieldSelector=spec.nodeName%3Dapi-server-failure-worker2&resourceVersion=1006&timeoutSeconds=442&watch=true": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:07 api-server-failure-worker2 kubelet[228]: E1228 16:09:07.683754     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: Get "https://api-server-failure-control-plane:6443/api/v1/nodes?allowWatchBookmarks=true&fieldSelector=metadata.name%3Dapi-server-failure-worker2&resourceVersion=1003&timeout=9m15s&timeoutSeconds=555&watch=true": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:07 api-server-failure-worker2 kubelet[228]: E1228 16:09:07.754981     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: Get "https://api-server-failure-control-plane:6443/apis/storage.k8s.io/v1/csidrivers?allowWatchBookmarks=true&resourceVersion=1006&timeout=6m5s&timeoutSeconds=365&watch=true": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:09 api-server-failure-worker2 kubelet[228]: W1228 16:09:09.718739     228 reflector.go:547] object-"kube-system"/"kube-proxy": failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:09 api-server-failure-worker2 kubelet[228]: E1228 16:09:09.718818     228 reflector.go:150] object-"kube-system"/"kube-proxy": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:09 api-server-failure-worker2 kubelet[228]: W1228 16:09:09.890298     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://api-server-failure-control-plane:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:09 api-server-failure-worker2 kubelet[228]: E1228 16:09:09.890390     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://api-server-failure-control-plane:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:10 api-server-failure-worker2 kubelet[228]: W1228 16:09:10.011852     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://api-server-failure-control-plane:6443/api/v1/services?resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:10 api-server-failure-worker2 kubelet[228]: E1228 16:09:10.011954     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://api-server-failure-control-plane:6443/api/v1/services?resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:10 api-server-failure-worker2 kubelet[228]: W1228 16:09:10.103008     228 reflector.go:547] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:10 api-server-failure-worker2 kubelet[228]: E1228 16:09:10.103155     228 reflector.go:150] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:10 api-server-failure-worker2 kubelet[228]: W1228 16:09:10.305546     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://api-server-failure-control-plane:6443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:10 api-server-failure-worker2 kubelet[228]: E1228 16:09:10.305586     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://api-server-failure-control-plane:6443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:10 api-server-failure-worker2 kubelet[228]: E1228 16:09:10.328613     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
Dec 28 16:09:10 api-server-failure-worker2 kubelet[228]: W1228 16:09:10.452954     228 reflector.go:547] pkg/kubelet/config/apiserver.go:66: failed to list *v1.Pod: Get "https://api-server-failure-control-plane:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dapi-server-failure-worker2&resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:10 api-server-failure-worker2 kubelet[228]: E1228 16:09:10.453030     228 reflector.go:150] pkg/kubelet/config/apiserver.go:66: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://api-server-failure-control-plane:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dapi-server-failure-worker2&resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:10 api-server-failure-worker2 kubelet[228]: W1228 16:09:10.734963     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://api-server-failure-control-plane:6443/api/v1/nodes?fieldSelector=metadata.name%3Dapi-server-failure-worker2&resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:10 api-server-failure-worker2 kubelet[228]: E1228 16:09:10.735033     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://api-server-failure-control-plane:6443/api/v1/nodes?fieldSelector=metadata.name%3Dapi-server-failure-worker2&resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:14 api-server-failure-worker2 kubelet[228]: W1228 16:09:14.522564     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://api-server-failure-control-plane:6443/api/v1/services?resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:14 api-server-failure-worker2 kubelet[228]: E1228 16:09:14.522619     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://api-server-failure-control-plane:6443/api/v1/services?resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:14 api-server-failure-worker2 kubelet[228]: W1228 16:09:14.808445     228 reflector.go:547] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:14 api-server-failure-worker2 kubelet[228]: E1228 16:09:14.808492     228 reflector.go:150] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:14 api-server-failure-worker2 kubelet[228]: W1228 16:09:14.816223     228 reflector.go:547] object-"kube-system"/"kube-proxy": failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:14 api-server-failure-worker2 kubelet[228]: E1228 16:09:14.816289     228 reflector.go:150] object-"kube-system"/"kube-proxy": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:14 api-server-failure-worker2 kubelet[228]: W1228 16:09:14.966834     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://api-server-failure-control-plane:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:14 api-server-failure-worker2 kubelet[228]: E1228 16:09:14.967011     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://api-server-failure-control-plane:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:14 api-server-failure-worker2 kubelet[228]: W1228 16:09:14.966894     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://api-server-failure-control-plane:6443/api/v1/nodes?fieldSelector=metadata.name%3Dapi-server-failure-worker2&resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:14 api-server-failure-worker2 kubelet[228]: E1228 16:09:14.967055     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://api-server-failure-control-plane:6443/api/v1/nodes?fieldSelector=metadata.name%3Dapi-server-failure-worker2&resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:15 api-server-failure-worker2 kubelet[228]: W1228 16:09:15.904789     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://api-server-failure-control-plane:6443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:15 api-server-failure-worker2 kubelet[228]: E1228 16:09:15.904875     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://api-server-failure-control-plane:6443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:16 api-server-failure-worker2 kubelet[228]: W1228 16:09:16.656835     228 reflector.go:547] pkg/kubelet/config/apiserver.go:66: failed to list *v1.Pod: Get "https://api-server-failure-control-plane:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dapi-server-failure-worker2&resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:16 api-server-failure-worker2 kubelet[228]: E1228 16:09:16.656941     228 reflector.go:150] pkg/kubelet/config/apiserver.go:66: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://api-server-failure-control-plane:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dapi-server-failure-worker2&resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:17 api-server-failure-worker2 kubelet[228]: E1228 16:09:17.340969     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
Dec 28 16:09:17 api-server-failure-worker2 kubelet[228]: E1228 16:09:17.574155     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?resourceVersion=0&timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:17 api-server-failure-worker2 kubelet[228]: E1228 16:09:17.581055     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:17 api-server-failure-worker2 kubelet[228]: E1228 16:09:17.586771     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:17 api-server-failure-worker2 kubelet[228]: E1228 16:09:17.593739     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:17 api-server-failure-worker2 kubelet[228]: E1228 16:09:17.598760     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:17 api-server-failure-worker2 kubelet[228]: E1228 16:09:17.598796     228 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
Dec 28 16:09:22 api-server-failure-worker2 kubelet[228]: W1228 16:09:22.685529     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://api-server-failure-control-plane:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:22 api-server-failure-worker2 kubelet[228]: E1228 16:09:22.685699     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://api-server-failure-control-plane:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:24 api-server-failure-worker2 kubelet[228]: W1228 16:09:24.197974     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://api-server-failure-control-plane:6443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:24 api-server-failure-worker2 kubelet[228]: E1228 16:09:24.198161     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://api-server-failure-control-plane:6443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:24 api-server-failure-worker2 kubelet[228]: E1228 16:09:24.350575     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
Dec 28 16:09:25 api-server-failure-worker2 kubelet[228]: W1228 16:09:25.274253     228 reflector.go:547] pkg/kubelet/config/apiserver.go:66: failed to list *v1.Pod: Get "https://api-server-failure-control-plane:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dapi-server-failure-worker2&resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:25 api-server-failure-worker2 kubelet[228]: E1228 16:09:25.274372     228 reflector.go:150] pkg/kubelet/config/apiserver.go:66: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://api-server-failure-control-plane:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dapi-server-failure-worker2&resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:26 api-server-failure-worker2 kubelet[228]: W1228 16:09:26.506439     228 reflector.go:547] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:26 api-server-failure-worker2 kubelet[228]: E1228 16:09:26.506612     228 reflector.go:150] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:26 api-server-failure-worker2 kubelet[228]: W1228 16:09:26.849191     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://api-server-failure-control-plane:6443/api/v1/nodes?fieldSelector=metadata.name%3Dapi-server-failure-worker2&resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:26 api-server-failure-worker2 kubelet[228]: E1228 16:09:26.849361     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://api-server-failure-control-plane:6443/api/v1/nodes?fieldSelector=metadata.name%3Dapi-server-failure-worker2&resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:27 api-server-failure-worker2 kubelet[228]: W1228 16:09:27.161747     228 reflector.go:547] object-"kube-system"/"kube-proxy": failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:27 api-server-failure-worker2 kubelet[228]: E1228 16:09:27.161927     228 reflector.go:150] object-"kube-system"/"kube-proxy": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:27 api-server-failure-worker2 kubelet[228]: W1228 16:09:27.193718     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://api-server-failure-control-plane:6443/api/v1/services?resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:27 api-server-failure-worker2 kubelet[228]: E1228 16:09:27.193844     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://api-server-failure-control-plane:6443/api/v1/services?resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:27 api-server-failure-worker2 kubelet[228]: E1228 16:09:27.861366     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?resourceVersion=0&timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:27 api-server-failure-worker2 kubelet[228]: E1228 16:09:27.866960     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:27 api-server-failure-worker2 kubelet[228]: E1228 16:09:27.872196     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:27 api-server-failure-worker2 kubelet[228]: E1228 16:09:27.875496     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:27 api-server-failure-worker2 kubelet[228]: E1228 16:09:27.878262     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:27 api-server-failure-worker2 kubelet[228]: E1228 16:09:27.878294     228 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
Dec 28 16:09:31 api-server-failure-worker2 kubelet[228]: E1228 16:09:31.359875     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
Dec 28 16:09:38 api-server-failure-worker2 kubelet[228]: E1228 16:09:38.097054     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?resourceVersion=0&timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:38 api-server-failure-worker2 kubelet[228]: E1228 16:09:38.101720     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:38 api-server-failure-worker2 kubelet[228]: E1228 16:09:38.107379     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:38 api-server-failure-worker2 kubelet[228]: E1228 16:09:38.111608     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:38 api-server-failure-worker2 kubelet[228]: E1228 16:09:38.114536     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:38 api-server-failure-worker2 kubelet[228]: E1228 16:09:38.114565     228 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
Dec 28 16:09:38 api-server-failure-worker2 kubelet[228]: E1228 16:09:38.366069     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
Dec 28 16:09:40 api-server-failure-worker2 kubelet[228]: W1228 16:09:40.440290     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://api-server-failure-control-plane:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:40 api-server-failure-worker2 kubelet[228]: E1228 16:09:40.440437     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://api-server-failure-control-plane:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:44 api-server-failure-worker2 kubelet[228]: W1228 16:09:44.894547     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://api-server-failure-control-plane:6443/api/v1/nodes?fieldSelector=metadata.name%3Dapi-server-failure-worker2&resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:44 api-server-failure-worker2 kubelet[228]: E1228 16:09:44.894723     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://api-server-failure-control-plane:6443/api/v1/nodes?fieldSelector=metadata.name%3Dapi-server-failure-worker2&resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:45 api-server-failure-worker2 kubelet[228]: W1228 16:09:45.224923     228 reflector.go:547] object-"kube-system"/"kube-proxy": failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:45 api-server-failure-worker2 kubelet[228]: E1228 16:09:45.225115     228 reflector.go:150] object-"kube-system"/"kube-proxy": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:45 api-server-failure-worker2 kubelet[228]: E1228 16:09:45.377532     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
Dec 28 16:09:48 api-server-failure-worker2 kubelet[228]: E1228 16:09:48.197933     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?resourceVersion=0&timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:48 api-server-failure-worker2 kubelet[228]: E1228 16:09:48.202967     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:48 api-server-failure-worker2 kubelet[228]: E1228 16:09:48.207807     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:48 api-server-failure-worker2 kubelet[228]: E1228 16:09:48.211591     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:48 api-server-failure-worker2 kubelet[228]: E1228 16:09:48.215590     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:48 api-server-failure-worker2 kubelet[228]: E1228 16:09:48.215618     228 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
Dec 28 16:09:48 api-server-failure-worker2 kubelet[228]: W1228 16:09:48.975884     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://api-server-failure-control-plane:6443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:48 api-server-failure-worker2 kubelet[228]: E1228 16:09:48.976085     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://api-server-failure-control-plane:6443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:49 api-server-failure-worker2 kubelet[228]: W1228 16:09:49.907310     228 reflector.go:547] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:49 api-server-failure-worker2 kubelet[228]: E1228 16:09:49.907467     228 reflector.go:150] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:50 api-server-failure-worker2 kubelet[228]: W1228 16:09:50.370275     228 reflector.go:547] pkg/kubelet/config/apiserver.go:66: failed to list *v1.Pod: Get "https://api-server-failure-control-plane:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dapi-server-failure-worker2&resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:50 api-server-failure-worker2 kubelet[228]: E1228 16:09:50.370437     228 reflector.go:150] pkg/kubelet/config/apiserver.go:66: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://api-server-failure-control-plane:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dapi-server-failure-worker2&resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:52 api-server-failure-worker2 kubelet[228]: W1228 16:09:52.067735     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://api-server-failure-control-plane:6443/api/v1/services?resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:52 api-server-failure-worker2 kubelet[228]: E1228 16:09:52.067811     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://api-server-failure-control-plane:6443/api/v1/services?resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:09:52 api-server-failure-worker2 kubelet[228]: E1228 16:09:52.384878     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
Dec 28 16:09:58 api-server-failure-worker2 kubelet[228]: E1228 16:09:58.510355     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?resourceVersion=0&timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:58 api-server-failure-worker2 kubelet[228]: E1228 16:09:58.513926     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:58 api-server-failure-worker2 kubelet[228]: E1228 16:09:58.516443     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:58 api-server-failure-worker2 kubelet[228]: E1228 16:09:58.519316     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:58 api-server-failure-worker2 kubelet[228]: E1228 16:09:58.521257     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:09:58 api-server-failure-worker2 kubelet[228]: E1228 16:09:58.521278     228 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
Dec 28 16:09:59 api-server-failure-worker2 kubelet[228]: E1228 16:09:59.394274     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
Dec 28 16:10:06 api-server-failure-worker2 kubelet[228]: E1228 16:10:06.400005     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
Dec 28 16:10:08 api-server-failure-worker2 kubelet[228]: E1228 16:10:08.903657     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?resourceVersion=0&timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:08 api-server-failure-worker2 kubelet[228]: E1228 16:10:08.909491     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:08 api-server-failure-worker2 kubelet[228]: E1228 16:10:08.914629     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:08 api-server-failure-worker2 kubelet[228]: E1228 16:10:08.920276     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:08 api-server-failure-worker2 kubelet[228]: E1228 16:10:08.925670     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:08 api-server-failure-worker2 kubelet[228]: E1228 16:10:08.925705     228 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
Dec 28 16:10:13 api-server-failure-worker2 kubelet[228]: E1228 16:10:13.408446     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
Dec 28 16:10:19 api-server-failure-worker2 kubelet[228]: E1228 16:10:19.069632     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?resourceVersion=0&timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:19 api-server-failure-worker2 kubelet[228]: E1228 16:10:19.075656     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:19 api-server-failure-worker2 kubelet[228]: E1228 16:10:19.081054     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:19 api-server-failure-worker2 kubelet[228]: E1228 16:10:19.086451     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:19 api-server-failure-worker2 kubelet[228]: E1228 16:10:19.090553     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:19 api-server-failure-worker2 kubelet[228]: E1228 16:10:19.090589     228 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
Dec 28 16:10:20 api-server-failure-worker2 kubelet[228]: E1228 16:10:20.416533     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
Dec 28 16:10:21 api-server-failure-worker2 kubelet[228]: W1228 16:10:21.125598     228 reflector.go:547] object-"kube-system"/"kube-root-ca.crt": failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:21 api-server-failure-worker2 kubelet[228]: E1228 16:10:21.125780     228 reflector.go:150] object-"kube-system"/"kube-root-ca.crt": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-root-ca.crt&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:21 api-server-failure-worker2 kubelet[228]: W1228 16:10:21.669675     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://api-server-failure-control-plane:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:21 api-server-failure-worker2 kubelet[228]: E1228 16:10:21.669800     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://api-server-failure-control-plane:6443/apis/node.k8s.io/v1/runtimeclasses?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:24 api-server-failure-worker2 kubelet[228]: W1228 16:10:24.779586     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://api-server-failure-control-plane:6443/api/v1/nodes?fieldSelector=metadata.name%3Dapi-server-failure-worker2&resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:24 api-server-failure-worker2 kubelet[228]: E1228 16:10:24.779882     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://api-server-failure-control-plane:6443/api/v1/nodes?fieldSelector=metadata.name%3Dapi-server-failure-worker2&resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:27 api-server-failure-worker2 kubelet[228]: E1228 16:10:27.425862     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
Dec 28 16:10:29 api-server-failure-worker2 kubelet[228]: E1228 16:10:29.127411     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?resourceVersion=0&timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:29 api-server-failure-worker2 kubelet[228]: E1228 16:10:29.132283     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:29 api-server-failure-worker2 kubelet[228]: E1228 16:10:29.136276     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:29 api-server-failure-worker2 kubelet[228]: E1228 16:10:29.140327     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:29 api-server-failure-worker2 kubelet[228]: E1228 16:10:29.145635     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:29 api-server-failure-worker2 kubelet[228]: E1228 16:10:29.145675     228 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
Dec 28 16:10:31 api-server-failure-worker2 kubelet[228]: W1228 16:10:31.980576     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://api-server-failure-control-plane:6443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:31 api-server-failure-worker2 kubelet[228]: E1228 16:10:31.980732     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://api-server-failure-control-plane:6443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:34 api-server-failure-worker2 kubelet[228]: E1228 16:10:34.438642     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
Dec 28 16:10:34 api-server-failure-worker2 kubelet[228]: W1228 16:10:34.771587     228 reflector.go:547] object-"kube-system"/"kube-proxy": failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:34 api-server-failure-worker2 kubelet[228]: E1228 16:10:34.771753     228 reflector.go:150] object-"kube-system"/"kube-proxy": Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://api-server-failure-control-plane:6443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dkube-proxy&resourceVersion=1004": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:35 api-server-failure-worker2 kubelet[228]: W1228 16:10:35.608396     228 reflector.go:547] pkg/kubelet/config/apiserver.go:66: failed to list *v1.Pod: Get "https://api-server-failure-control-plane:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dapi-server-failure-worker2&resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:35 api-server-failure-worker2 kubelet[228]: E1228 16:10:35.608575     228 reflector.go:150] pkg/kubelet/config/apiserver.go:66: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://api-server-failure-control-plane:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dapi-server-failure-worker2&resourceVersion=1006": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:39 api-server-failure-worker2 kubelet[228]: E1228 16:10:39.309160     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?resourceVersion=0&timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:39 api-server-failure-worker2 kubelet[228]: E1228 16:10:39.314581     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:39 api-server-failure-worker2 kubelet[228]: E1228 16:10:39.320161     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:39 api-server-failure-worker2 kubelet[228]: E1228 16:10:39.325127     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:39 api-server-failure-worker2 kubelet[228]: E1228 16:10:39.331402     228 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"api-server-failure-worker2\": Get \"https://api-server-failure-control-plane:6443/api/v1/nodes/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host"
Dec 28 16:10:39 api-server-failure-worker2 kubelet[228]: E1228 16:10:39.331488     228 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
Dec 28 16:10:39 api-server-failure-worker2 kubelet[228]: W1228 16:10:39.864010     228 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://api-server-failure-control-plane:6443/api/v1/services?resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:39 api-server-failure-worker2 kubelet[228]: E1228 16:10:39.864126     228 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://api-server-failure-control-plane:6443/api/v1/services?resourceVersion=1003": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host
Dec 28 16:10:41 api-server-failure-worker2 kubelet[228]: E1228 16:10:41.447771     228 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://api-server-failure-control-plane:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/api-server-failure-worker2?timeout=10s\": dial tcp: lookup api-server-failure-control-plane on 192.168.65.254:53: no such host" interval="7s"
